\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Bias in AI Scientific Report}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
}

\maketitle

\begin{abstract}

\end{abstract}

\begin{IEEEkeywords}

\end{IEEEkeywords}

\section{Project proposal}

What I'd like to do for my course project is to analyse the human-centric German-Credit dataset for assessing credit risk \cite{}.

What I intend to implement is one of the two methods of modifying data that \cite{Feldman2015ComputationalFP} proposes, called Combinatorial
and Geometric repair. 

The motivation for this project is to replicate the paper's claim that the repair performs favourably in terms of training classifiers that are
both accurate and unbiased.

The reasons I believe that the implementation suits this submodule as bias in artificial intelligence is that it lets me develop a fair machine learning ecosystem
to detect, reduce, and eventually mitigate different types of bias (such as the “80\% rule” of disparate impact) that exist in the final outcome of the algorithm in various ways

The concrete tasks I plan to do are as follows:
\begin{enumerate}
    \item Perform "cleaning", binning, bucketing, and discrete-to-continuous feature transformations on the dataset.
    \item Split the dataset into different demographic groups, and compute and report information for each feature for each group.
    \item Observe any interesting differences between the different
    subgroups’ statistics, and describe any bias observed and explain the reasons why this bias has happened from my point of view.
    \item Describe my chosen conventional algorithm to implement on the biased dataset, as well as describe the justification for selection.
    \item Naively split the dataset into training and testing sets.
    \item Train the model and see how it generalises to the testing dataset, as well as explain my approach and findings.
    \item Subsample a new testing dataset in an unbiased way, then retrain the model and see how it generalises to these new testing conditions. 
    \item Compare my findings with the previous results and explain my approach.
    \item Describe any sort of bias I observed, and explain the reasons why this bias has happened from my point of view.
    \item Implement one of Combinatorial or Geometric repair, and describe the algorithm.
    \item Test the performance of the trained model for the minority groups, and compare it with the performance of the model over the majority group.
    \item Describe any reduction in algorithmic bias I observe.
    \item State whether I get roughly the same results as the project paper, and, if not, reconsider my code or justify the reasons.
    \item Describe from my point of view any sort of reduction in accuracy I observe.
\end{enumerate}

The final work product of these tasks will be appropriate and proper plots and graphs that demonstrate any reduction in algorithmic bias and any sort of reduction in accuracy I observe after the fair machine learning implementation.

As I formulate this project, the particular context I'm thinking about is applying for credit loans.

The technologies I plan to use in my implementation are as follows: the Python programming language, the Pandas package for data analysis, and the Scikit-learn package for the implementation of the AI algorithm.
\newpage 

\section{Project progress report}
\subsection{Data analysis}
These tables need fixing.
\begin{center}
    \begin{tabular}{ |c|c|c| } 
        \hline
        & Age & cell3 \\ 
        \hline
        size     & 1000 & cell \\ 
        average  & 35.54 & cell \\ 
        variance & cell8 & cell \\ 
        \hline
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{ |c|c|c| } 
        \hline
                        & Sex   \\ 
        \hline
        size               & 1000  \\ 
        mode               & 35.54 \\ 
        most frequent      & cell8 \\ 
        2nd-most frequent  & cell8 \\ 
        3rd-most frequent  & cell8 \\ 
        \hline
    \end{tabular}
\end{center}

\subsection{Conventional implementation}
At this stage of the project, I have a biased dataset, and implement a conventional ML algorithm.

My chosen algorithm is \dots

The justification for selecting this algorithm is \dots

My approach is to split the dataset into training and testing sets by randomly sampling some of the data, with 70\% for training and 30\% for testing, followed by training the model on the training dataset, and then seeing how it generalises to the testing dataset. 

My findings were \dots

My next approach was to subsample a new testing dataset in an unbiased way, by ensuring gender and age diversity by doing \dots, followed by retraining the model on the training dataset, and then seeing how it generalises to the testing dataset with new conditions.

My new findings were \dots, and compared to the previous results, they are \dots.

I observed there to be bias in \dots and \dots. From my point of view, the reason why this bias has happened is because \dots.

\subsection{Fair machine learning implementation}
In this step, I implement one of the fair ML methods of mitigating bias. 

The solutions that were provided in the suggested project paper \cite{Feldman2015ComputationalFP} are two methods of modifying data, called Combinatorial and Geometric repair. We choose only one algorithm to implement due to time constraints brought about by other academic commitments. 

The proposed algorithm, combinatorial repair, is \dots

We test the performance of our trained model for the minority groups. Compared with the performance of our model over the majority group, \dots

We observe a reduction in algorithmic bias, which is \dots. We plot the results demonstrating this below:

We do get roughly the same results as in the project paper. We justify the reasons by \dots

\bibliographystyle{plain}
\bibliography{references}
\end{document}
