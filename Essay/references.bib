@article{DBLP:journals/corr/abs-1908-09635,
  author    = {Ninareh Mehrabi and
               Fred Morstatter and
               Nripsuta Saxena and
               Kristina Lerman and
               Aram Galstyan},
  title     = {A Survey on Bias and Fairness in Machine Learning},
  journal   = {CoRR},
  volume    = {abs/1908.09635},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.09635},
  archivePrefix = {arXiv},
  eprint    = {1908.09635},
  timestamp = {Thu, 29 Aug 2019 16:32:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-09635.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{10.2307/2284382,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2284382},
 abstract = {This paradox is the possibility of $P(A \mid B) < P(A \mid B\prime)$ even though P(A ∣ B) ≥ P(A ∣ B′) both under the additional condition C and under the complement C′ of that condition. Details are given on why this can happen and how extreme the inequalities can be. An example shows that Savage's sure-thing principle ("If you would definitely prefer g to f, either knowing that the event C obtained, or knowing that C did not obtain, then you definitely prefer g to f.") is not applicable to alternatives f and g that involve sequential operations.},
 author = {Colin R. Blyth},
 journal = {Journal of the American Statistical Association},
 number = {338},
 pages = {364--366},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {On Simpson's Paradox and the Sure-Thing Principle},
 volume = {67},
 year = {1972}
}

@article {Bickel398,
	author = {Bickel, P. J. and Hammel, E. A. and O{\textquoteright}Connell, J. W.},
	title = {Sex Bias in Graduate Admissions: Data from Berkeley},
	volume = {187},
	number = {4175},
	pages = {398--404},
	year = {1975},
	doi = {10.1126/science.187.4175.398},
	publisher = {American Association for the Advancement of Science},
	abstract = {Examination of aggregate data on graduate admissions to the University of California, Berkeley, for fall 1973 shows a clear but misleading pattern of bias against female applicants. Examination of the disaggregated data reveals few decision-making units that show statistically significant departures from expected frequencies of female admissions, and about as many units appear to favor women as to favor men. If the data are properly pooled, taking into account the autonomy of departmental decision making, thus correcting for the tendency of women to apply to graduate departments that are more difficult for applicants of either sex to enter, there is a small but statistically significant bias in favor of women. The graduate departments that are easier to enter tend to be those that require more mathematics in the undergraduate preparatory curriculum. The bias in the aggregated data stems not from any pattern of discrimination on the part of admissions committees, which seem quite fair on the whole, but apparently from prior screening at earlier levels of the educational system. Women are shunted by their socialization and education toward fields of graduate study that are generally more crowded, less productive of completed degrees, and less well funded, and that frequently offer poorer professional employment prospects.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/187/4175/398},
	eprint = {https://science.sciencemag.org/content/187/4175/398.full.pdf},
	journal = {Science}
}


@article{DBLP:journals/corr/abs-1901-10002,
  author    = {Harini Suresh and
               John V. Guttag},
  title     = {A Framework for Understanding Unintended Consequences of Machine Learning},
  journal   = {CoRR},
  volume    = {abs/1901.10002},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.10002},
  archivePrefix = {arXiv},
  eprint    = {1901.10002},
  timestamp = {Sat, 02 Feb 2019 16:56:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-10002.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{GrgicHlaca2016TheCF,
  title={The Case for Process Fairness in Learning: Feature Selection for Fair Decision Making},
  author={Nina Grgic-Hlaca and M. Zafar and K. Gummadi and Adrian Weller},
  year={2016}
}

@inproceedings{NIPS2017_a486cd07,
 author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {4066--4076},
 publisher = {Curran Associates, Inc.},
 title = {Counterfactual Fairness},
 url = {https://proceedings.neurips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{Chen_2019,
   title={Fairness Under Unawareness},
   ISBN={9781450361255},
   url={http://dx.doi.org/10.1145/3287560.3287594},
   DOI={10.1145/3287560.3287594},
   journal={Proceedings of the Conference on Fairness, Accountability, and Transparency},
   publisher={ACM},
   author={Chen, Jiahao and Kallus, Nathan and Mao, Xiaojie and Svacha, Geoffry and Udell, Madeleine},
   year={2019},
   month={Jan}
}

@inproceedings{10.1145/3194770.3194776,
author = {Verma, Sahil and Rubin, Julia},
title = {Fairness Definitions Explained},
year = {2018},
isbn = {9781450357463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194770.3194776},
doi = {10.1145/3194770.3194776},
abstract = {Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
booktitle = {Proceedings of the International Workshop on Software Fairness},
pages = {1–7},
numpages = {7},
location = {Gothenburg, Sweden},
series = {FairWare '18}
}

@article{DBLP:journals/corr/abs-1811-05577,
  author    = {Pedro Saleiro and
               Benedict Kuester and
               Abby Stevens and
               Ari Anisfeld and
               Loren Hinkson and
               Jesse London and
               Rayid Ghani},
  title     = {Aequitas: {A} Bias and Fairness Audit Toolkit},
  journal   = {CoRR},
  volume    = {abs/1811.05577},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.05577},
  archivePrefix = {arXiv},
  eprint    = {1811.05577},
  timestamp = {Sat, 24 Nov 2018 17:52:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-05577.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1801-04385,
  author    = {Nazanin Alipourfard and
               Peter G. Fennell and
               Kristina Lerman},
  title     = {Can you Trust the Trend: Discovering Simpson's Paradoxes in Social
               Data},
  journal   = {CoRR},
  volume    = {abs/1801.04385},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.04385},
  archivePrefix = {arXiv},
  eprint    = {1801.04385},
  timestamp = {Mon, 13 Aug 2018 16:47:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-04385.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1805-03094,
  author    = {Nazanin Alipourfard and
               Peter G. Fennell and
               Kristina Lerman},
  title     = {Using Simpson's Paradox to Discover Interesting Patterns in Behavioral
               Data},
  journal   = {CoRR},
  volume    = {abs/1805.03094},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.03094},
  archivePrefix = {arXiv},
  eprint    = {1805.03094},
  timestamp = {Mon, 13 Aug 2018 16:48:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-03094.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/HardtPS16,
  author    = {Moritz Hardt and
               Eric Price and
               Nathan Srebro},
  title     = {Equality of Opportunity in Supervised Learning},
  journal   = {CoRR},
  volume    = {abs/1610.02413},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.02413},
  archivePrefix = {arXiv},
  eprint    = {1610.02413},
  timestamp = {Mon, 13 Aug 2018 16:47:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HardtPS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{d_Alessandro_2017,
   title={Conscientious Classification: A Data Scientist’s Guide to Discrimination-Aware Classification},
   volume={5},
   ISSN={2167-647X},
   url={http://dx.doi.org/10.1089/big.2016.0048},
   DOI={10.1089/big.2016.0048},
   number={2},
   journal={Big Data},
   publisher={Mary Ann Liebert Inc},
   author={d’ Alessandro, Brian and O’Neil, Cathy and LaGatta, Tom},
   year={2017},
   month={Jun},
   pages={120–134}
}

@article{10.2307/24758720,
 ISSN = {00081221},
 URL = {http://www.jstor.org/stable/24758720},
 abstract = {Advocates of algorithmic techniques like data mining argue that these techniques eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data is frequently imperfect in ways that allow these algorithms to inherit the prejudices of prior decision makers. In other cases, data may simply reflect the widespread biases that persist in society at large. In still others, data mining can discover surprisingly useful regularities that are really just preexisting patterns of exclusion and inequality. Unthinking reliance on data mining can deny historically disadvantaged and vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm's use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court. This Essay examines these concerns through the lens of American antidiscrimination law—more particularly, through Title VII's prohibition of discrimination in employment. In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining's victims would seem to lie in disparate impact doctrine. Case law and the Equal Employment Opportunity Commission's Uniform Guidelines, though, hold that a practice can be justified as a business necessity when its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. Unless there is a reasonably practical way to demonstrate that these discoveries are spurious, Title VII would appear to bless its use, even though the correlations it discovers will often reflect historic patterns of prejudice, others' discrimination against members of protected groups, or flaws in the underlying data. Addressing the sources of this unintentional discrimination and remedying the corresponding deficiencies in the law will be difficult technically, difficult legally, and difficult politically. There are a number of practical limits to what can be accomplished computationally. For example, when discrimination occurs because the data being mined is itself a result of past intentional discrimination, there is frequently no obvious method to adjust historical data to rid it of this taint. Corrective measures that alter the results of the data mining after it is complete would tread on legally and politically disputed terrain. These challenges for reform throw into stark relief the tension between the two major theories underlying antidiscrimination law: anticlassification and antisubordination. Finding a solution to big data's disparate impact will require more than best efforts to stamp out prejudice and bias; it will require a wholesale reexamination of the meanings of "discrimination" and "fairness."},
 author = {Solon Barocas and Andrew D. Selbst},
 journal = {California Law Review},
 number = {3},
 pages = {671--732},
 publisher = {California Law Review, Inc.},
 title = {Big Data's Disparate Impact},
 volume = {104},
 year = {2016}
}

@article{DBLP:journals/corr/Zliobaite15a,
  author    = {Indre Zliobaite},
  title     = {A survey on measuring indirect discrimination in machine learning},
  journal   = {CoRR},
  volume    = {abs/1511.00148},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.00148},
  archivePrefix = {arXiv},
  eprint    = {1511.00148},
  timestamp = {Mon, 13 Aug 2018 16:48:19 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Zliobaite15a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{article,
author = {Zemel, R. and Wu, Y. and Swersky, K. and Pitassi, T. and Dwork, C.},
year = {2013},
month = {01},
pages = {1362-1370},
title = {Learning fair representations},
journal = {30th International Conference on Machine Learning, ICML 2013}
}

@article{DBLP:journals/corr/Zliobaite15,
  author    = {Indre Zliobaite},
  title     = {On the relation between accuracy and fairness in binary classification},
  journal   = {CoRR},
  volume    = {abs/1505.05723},
  year      = {2015},
  url       = {http://arxiv.org/abs/1505.05723},
  archivePrefix = {arXiv},
  eprint    = {1505.05723},
  timestamp = {Mon, 13 Aug 2018 16:48:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Zliobaite15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/1401890.1401959,
author = {Pedreshi, Dino and Ruggieri, Salvatore and Turini, Franco},
title = {Discrimination-Aware Data Mining},
year = {2008},
isbn = {9781605581934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1401890.1401959},
doi = {10.1145/1401890.1401959},
abstract = {In the context of civil rights law, discrimination refers to unfair or unequal treatment of people based on membership to a category or a minority, without regard to individual merit. Rules extracted from databases by data mining techniques, such as classification or association rules, when used for decision tasks such as benefit or credit approval, can be discriminatory in the above sense. In this paper, the notion of discriminatory classification rules is introduced and studied. Providing a guarantee of non-discrimination is shown to be a non trivial task. A naive approach, like taking away all discriminatory attributes, is shown to be not enough when other background knowledge is available. Our approach leads to a precise formulation of the redlining problem along with a formal result relating discriminatory rules with apparently safe ones by means of background knowledge. An empirical assessment of the results on the German credit dataset is also provided.},
booktitle = {Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {560–568},
numpages = {9},
keywords = {discrimination, classification rules},
location = {Las Vegas, Nevada, USA},
series = {KDD '08}
}

@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@misc{aif360-oct-2018,
    title = "{AI Fairness} 360:  An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias",
    author = {Rachel K. E. Bellamy and Kuntal Dey and Michael Hind and
	Samuel C. Hoffman and Stephanie Houde and Kalapriya Kannan and
	Pranay Lohia and Jacquelyn Martino and Sameep Mehta and
	Aleksandra Mojsilovic and Seema Nagar and Karthikeyan Natesan Ramamurthy and
	John Richards and Diptikalyan Saha and Prasanna Sattigeri and
	Moninder Singh and Kush R. Varshney and Yunfeng Zhang},
    month = oct,
    year = {2018},
    url = {https://arxiv.org/abs/1810.01943}
}