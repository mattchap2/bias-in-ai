\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Bias in AI Essay
}

% \author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% }

\maketitle

% \begin{abstract}

% \end{abstract}

% \begin{IEEEkeywords}

% \end{IEEEkeywords}

\section{Introduction}
This essay demonstrates our point of view on the overall subject of algorithmic bias. First, we discuss the justifications on the reasons that bias in AI-based solutions should be addressed. Second, we demonstrate various ways to measure the fairness of a dataset and algorithm. Third, we discuss different ways to mitigate algorithmic bias. Last of all, we discuss what we expect to see in the fair machine learning solutions in the future.

\section{Justifications on the reasons that bias in AI–based solutions should be addressed}
One reason to address bias in AI-based solutions is to ensure that the decisions made by the solutions do not reflect discriminatory behaviour toward certain groups or populations, as stated by \cite{DBLP:journals/corr/abs-1908-09635}. To justify this, they highlight the canonical example of the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) software, which was found to more likely assign a higher risk score (of recommitting another crime) to African-American offenders than to Caucasians with the same profile\footnote{propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}. 

Another reason that bias in AI-based solutions should be addressed is to avoid perpetuating any systemic discrimination, under a misleading veil of data-driven objectivity, as said by \cite{d_Alessandro_2017}. To justify this, they point to the broader debate concerning disparate impact, which is discussed extensively by \cite{10.2307/24758720}. They highlight `redlining', which is the refusal of opportunities to people base solely on their zip code, for loans as a classic example of disparate impact.

% \cite{10.1145/1401890.1401959} echoes the sentiment of the authors in the previous paragraph, and tackles the problem of discrimination in data mining in a rule-based setting, by introducing the notion of discriminatory classification rules, as a criterion to identify the potential risks of discrimination.

\section{Ways to measure the fairness of a dataset and algorithm}
\subsection{Ways to measure the fairness of a dataset}
One way to measure fairness in a dataset is to address Simpson's Paradox \cite{10.2307/2284382}. This can be done by comparing the regression for the entire population, regressions for each subgroup, and the unbiased regression. An example arose in \cite{Bickel398}, where it seemed like there was bias toward women in graduate school admissions, but, at the same time, women also had an advantage over men in some cases, such as when the data was separated and analysed over the departments.

Another way to measure fairness in a dataset, which is traditional and statistical, is selection bias \cite{DBLP:journals/corr/abs-1901-10002}. Selection bias can be measured by calculating the divergence of the probability distribution over the space of inputs in the training data against the true data distribution.

Other measures of fairness in a dataset include historical bias and representation bias. As described by \cite{DBLP:journals/corr/abs-1901-10002}, historical bias can be measured by evaluating the representational harm --- such as reinforcing a stereotype --- to a particular identity group, while representation bias can be measured by calculating the percentage a minority group makes up of the true distribution. 

\subsection{Ways to measure the fairness of an algorithm}
We can use definitions of fairness, many of which have been compiled by \cite{DBLP:journals/corr/abs-1908-09635}, as ways to measure the fairness of an algorithm.

One measure is \emph{Fairness Through Unawareness}, which says that `\textit{An algorithm is fair as long as any protected attributes are not explicitly used in the decision-making process}' \cite{GrgicHlaca2016TheCF, NIPS2017_a486cd07}. Protected attributes are specified in the Fair Housing and Equal Credit Opportunity Acts (FHA and ECOA) \cite{Chen_2019}, and include race, colour, national origin, religion, sex, and more.

Another measure is \emph{Demographic Parity}. This states that the likelihood of a positive outcome should be the same regardless of whether the person is in the protected group \cite{10.1145/3194770.3194776}. In mathematical terms, `\textit{A predictor $\hat{Y}$ satisfies demographic parity if $P(\hat{Y}|A=0) = P(\hat{Y}|A=1)$}', where $A$ is a protected attribute and $\hat{Y}$ is a binary predictor.  

Closely related are \textit{Equalised Odds} and \textit{Equal Opportunity}. The equalised odds definition states that the protected and unprotected groups should have equal rates for true positives and false positives (i.e., $P(\hat{Y}|A=0, Y=y) = P(\hat{Y}|A=1, Y=y), y \in {0,1}$); while, the equal opportunity definition states that the protected and unprotected groups should have equal true positive rates (i.e., $P(\hat{Y}|A=0, Y=1) = P(\hat{Y}|A=1, Y=1)$) \cite{DBLP:journals/corr/HardtPS16}.

Additional metrics measure disparate impact specifically, which are described in \cite{DBLP:journals/corr/Zliobaite15a} and summarised by \cite{d_Alessandro_2017}. The measure recommended is the \emph{Mean Difference} divided by a normalisation constant, which measures the difference between the means of the targets of the protected group and the general group, where no difference indicates no discrimination. The mean difference can be modified to obtain a \emph{Conditional Mean Difference}, which accounts for distributional differences between the protected populations and the overall population.

To tackle the problem of discrimination in data mining in a rule-based setting, \cite{10.1145/1401890.1401959} introduces the notion of discriminatory classification rules as a criterion to identify the potential risks of discrimination. (Strong) $\alpha$-protection is one such criterion that measures the discriminatory power of rules which occur in approaches such as decision trees and rule-based classifiers. An example of a potentially discriminatory (PD) rule they give, in the context of the German credit dataset \cite{Dua:2019}, is as follows: 

\begin{lstlisting}
    personal_status=female div/sep/mar 
        savings_status=no known savings 
        ==> class=bad,    
\end{lstlisting}

which contains the PD \emph{attribute personal\_status}.

\section{Ways to mitigate algorithmic bias}
There are a number of ways to mitigate algorithmic bias, which have also been compiled by \cite{DBLP:journals/corr/abs-1908-09635}.

Methods to discover Simpson’s paradoxes in data automatically have been proposed by \cite{DBLP:journals/corr/abs-1801-04385} and \cite{DBLP:journals/corr/abs-1805-03094}, while \cite{DBLP:journals/corr/HardtPS16} proposes a criterion to satisfy equalised odds and equal opportunity in supervised learning.

Another way to mitigate algorithmic bias is mentioned in \cite{d_Alessandro_2017}, which is to introduce augmented cost functions during the model training phase. In \cite{article} and \cite{DBLP:journals/corr/Zliobaite15}, both augment a standard log-likelihood loss function with a fairness regulariser which takes into account differences in how the learning algorithm classifies protected and non-protected classes.

The final way of mitigating algorithmic bias we mention is to enforce fairness criteria, such as \emph{Independence}, \emph{Separation}, and \emph{Sufficiency}.

\section{What we expect to see in the fair machine learning solutions in the future}
Recently, researchers have begun introducing tools that can assess the amount of fairness in a tool or system \cite{DBLP:journals/corr/abs-1908-09635}. One example is Aequitas \cite{DBLP:journals/corr/abs-1811-05577}, which lets users test models for different population subgroups. Others include IBM's AI Fairness 360 \cite{aif360-oct-2018} and Google's What-If Tool\footnote{github.com/PAIR-code/what-if-tool}. We expect to see more of these tools in the fair machine learning solutions in the future.

\bibliographystyle{plain}
\bibliography{references}
\end{document}
